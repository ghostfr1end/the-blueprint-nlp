{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mLjyYCcg66W"
      },
      "source": [
        "# Контент-анализ Telegram-канала с применением SOTA-инструментов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhjSTkErhW-J"
      },
      "source": [
        "Для контент-анализа был выбран telegram-канал **The Blueprint** – независимого\n",
        "онлайн-издания о моде, красоте и современной культуре.\n",
        "\n",
        "### Почему именно этот канал:\n",
        "- Канал публикует **регулярные текстовые посты**, что делает его релевантным источником данных для анализа, поскольку регулярность обеспечивает достаточный объём данных для анализа.\n",
        "- Содержимое канала отражает **актуальные культурные и потребительские тренды**. Такой контент позволяет с помощью современных NLP-инструментов выявлять латентные темы и анализировать поведение аудитории.\n",
        "- Это **публичный и активный источник** с качественно оформленным текстом, что упрощает обработку и повышает достоверность анализа.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOWVPu9UjCKT"
      },
      "source": [
        "### Цель проекта:\n",
        "Проанализировать содержание текстовых сообщений Telegram-канала The Blueprint в период с 1 марта по 31 мая, выявить ключевые темы, повторяющиеся паттерны и связи между постами, используя векторные представления текста и современные embedding-модели.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Инструменты:\n",
        "\n",
        "- sentence-transformers – модель paraphrase-multilingual-mpnet-base-v2 для эмбеддингов.\n",
        "\n",
        "- faiss-cpu – векторный индекс и поиск (cosine/IP с L2-нормированием).\n",
        "\n",
        "- numpy, pandas – подготовка данных, агрегации по неделям.\n",
        "\n",
        "- scikit-learn и TfidfVectorizer/CountVectorizer для извлечения и взвешивания n-грамм.\n",
        "\n",
        "- matplotlib – визуализации: бар-чарты, таймлайн, теплокарта.\n",
        "\n",
        "- re – регулярные выражения для тематических паттернов и стоп-слов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cpJw7OIvbJn"
      },
      "source": [
        "# Получение данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDLLl3mevqhK"
      },
      "source": [
        "- Импорт из: Telegram\n",
        "\n",
        "- Формат: JSON\n",
        "\n",
        "- Количество сообщений: 37438\n",
        "\n",
        "- Период: с 01.03.2025 по 31.05.2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SegAligXveAS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"result.json\", \"r\", encoding = \"utf-8\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "messages = []\n",
        "\n",
        "for elem in data[\"messages\"]:\n",
        "  if type(elem) == dict:\n",
        "    if type(elem[\"text\"]) == list:\n",
        "      message = ' '\n",
        "      for el in elem[\"text\"]:\n",
        "        if type(el) == str:\n",
        "          message += el\n",
        "        else:\n",
        "          message += el[\"text\"]\n",
        "      messages.append(message)\n",
        "  if type(elem) == str:\n",
        "    messages.append(elem)\n",
        "\n",
        "print(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmDqYtoZ6B4I"
      },
      "outputs": [],
      "source": [
        "with open(\"data.txt\", \"w\", encoding = \"utf-8\") as f:\n",
        "    f.writelines(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO3iScsh66Q-"
      },
      "source": [
        "- Были извлечены тексты из Telegram-канала **The Blueprint** и сохранены в формате 'txt'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SssmOWIz7LDf"
      },
      "source": [
        "# Создание векторной базы данных с помощью SentenceTransformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRhIaQh-kaqr"
      },
      "source": [
        "### Чистка текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzicHauxkiP9"
      },
      "source": [
        "Используя регулярные выражения, очистим текст.\n",
        "\n",
        "Так, в контексте настоящего кейса необходимо:\n",
        "- Удаление эмодзи и символов (не несут смысловой нагрузки);\n",
        "- Удаление ссылок (не важны для семантики, содержат бесполезные токены);\n",
        "- Удаление лишних пробелов и переносов строк;\n",
        "- Удаление повторов и хэштегов (могут мешать кластеризации).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Было принято решение не:\n",
        "\n",
        "- Приводить текст к нижнему регистру (модели sentence transformers обучены работать со смешенным регистром);\n",
        "- Удалять стоп-слова (они нужны для работы со смыслом в контексте)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2_wPGmIkexB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "with open(\"data.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "    texts = f.readlines()\n",
        "\n",
        "def clean_text(texts):\n",
        "    cleaned = []\n",
        "    for text in texts:\n",
        "      text = re.sub(r\"http\\S+\", \"\", text)  # удаляем ссылки\n",
        "      text = re.sub(r\"(The Blueprint|#\\w+)\", \"\", text) # удаление повторов и хэштегов\n",
        "      text = re.sub(r\"[^\\w\\s.,!?-]\", \"\", text)  # удаляем эмодзи и спецсимволы\n",
        "      text = re.sub(r\"\\s+\", \" \", text).strip()  # удаляем лишние пробелы\n",
        "      cleaned.append(text)\n",
        "    return cleaned\n",
        "\n",
        "cleaned_texts = clean_text(texts)\n",
        "\n",
        "with open(\"data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in cleaned_texts:\n",
        "        f.write(line + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxgob8vyl7uX"
      },
      "source": [
        "Теперь:\n",
        "- Удалены лишние символы: нет ссылок, эмодзи, спецзнаков;\n",
        "- Сохранены знаки препинания, что важно для дальнейшего анализа;\n",
        "- Сохранена читаемость, без нарушений структуры предложений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhv88A088UnW"
      },
      "source": [
        "### Деление на чанки\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH7rC4fUNlNY"
      },
      "source": [
        "Будет использоваться рекурсивный сплиттер, позволяющий делить текст с учётом особенностей синтаксиса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF264-ySEnmx"
      },
      "outputs": [],
      "source": [
        "with open(\"data.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "  texts = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gAooqEJEwVH"
      },
      "outputs": [],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS4p_bHKExOw"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gILsNCldGne1"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 100\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_text(texts)\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "  print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*40}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEnDoyacmE-r"
      },
      "source": [
        "- Размер чанков оптимален для моделей Sentence Transformers.\n",
        "- Чанки содержат связный, тематически однородный текст, оптимальный для смысловой векторизации.\n",
        "- Текст литературно оформлен, значит, модель сможет точнее извлечь семантику."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W6bOQwPaVGB"
      },
      "source": [
        "### Создание векторной базы данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLmzXQDJUY8N"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Загружаем модель для создания векторных представлений\n",
        "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "\n",
        "# Кодируем чанки с помощью готовой модели\n",
        "vectors = model.encode(\n",
        "    chunks,\n",
        "    normalize_embeddings=True,\n",
        "    batch_size=64,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioSFt02MK9DH"
      },
      "source": [
        "Создаём индекс с помощью FAISS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy9xZ1D6mJJv"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n",
        "import faiss\n",
        "\n",
        "# Вычисляем размерность вектора\n",
        "dimension = vectors.shape[1]\n",
        "# Создаем индекс, числовой \"каталог\" текстов\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "faiss_index.add(np.array(vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaMJg1xmpsv6"
      },
      "outputs": [],
      "source": [
        "query = \"Мода и тренды\" # Вводим запрос\n",
        "top_k = 15 # Выбираем, какое количество результатов нужно отобразить\n",
        "vector = model.encode([query]) # Кодируем вектор\n",
        "\n",
        "# Производим поиск\n",
        "distances, indices = faiss_index.search(vector, top_k)\n",
        "\n",
        "# Создаем каталог результатов\n",
        "results = []\n",
        "for i, idx in enumerate(indices[0]):\n",
        "        results.append({\n",
        "            \"chunk\": chunks[idx],\n",
        "            \"score\": float(distances[0][i])\n",
        "        })\n",
        "\n",
        "# Выводим первый результат с ответом на вопрос\n",
        "print(\"Top results for the query:\")\n",
        "for i, result in enumerate(results, 1):\n",
        "  print(f\"{i}. Chunk: {result['chunk']}\\nScore: {result['score']}\\n{'-'*40}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnJ-eyShUGw0"
      },
      "source": [
        "### **Так, из 15 чанков по запросу \"Мода и тренды\" максимально релевантными оказались:**\n",
        "\n",
        "3\\. Стратегии независимых брендов vs люкс на красных дорожках (сдвиг пиар-подходов, тренды продвижения).\n",
        "\n",
        "4\\. Топ-аксессуары с недель моды + бьюти-тренды AW’25/26.\n",
        "\n",
        "5\\. «Шаровары – главный тренд лета» (Chloé, Alaïa, Valentino).\n",
        "\n",
        "7\\. LV Resort 2026: мудборд + «проводные наушники» как модный аксессуар.\n",
        "\n",
        "8\\. Разнообразие в моде (Vogue, Black Style): срез индустриального тренда на инклюзию.\n",
        "\n",
        "9\\. «27 главных трендов сезона AW’25/26».\n",
        "\n",
        "11\\. Кризис высокой моды, доверие к брендам, Hermès/Brunello Cucinelli: макротренд индустрии.\n",
        "\n",
        "14\\. SS’25: «Grandma chic», шорты сезона – конкретные гардеробные тренды.\n",
        "\n",
        "15\\. Доля plus-size/ mid-size, инклюзия как устойчивый социальный тренд в индустрии.\n",
        "\n",
        "13\\. Влияние Blackpink на моду: культурный драйвер трендов.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###**Относительно релеватными:**\n",
        "\n",
        "1\\. Стрит-стайл 90/00-х и «назревает ли тренд?» – релевантно; далее «Твин Пикс», театр — не по теме.\n",
        "\n",
        "2\\. Джинсовые тренды (baggy/wide leg) релевантна; далее часть про Трейси Эмин.\n",
        "\n",
        "6\\. Дебюты креативных директоров – по теме; блок про Роберта Паттинсона выпадает.\n",
        "\n",
        "10\\. Тренд-репорт AW’25/26 – релевантно; архитектура и материал про седативы не подходят.\n",
        "\n",
        "12\\. Про новых креативных директоров – релевантно; «дофаминовые прыжки» не относятся к теме запроса.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYrKBwT-eGWQ"
      },
      "source": [
        "###**Выявленные паттерны**\n",
        "\n",
        "– Сезонные сводки трендов: регулярные обзоры коллекций AW’25/26 и SS’25 (одежда, аксессуары, бьюти), консолидирующие ключевые направления сезона.\n",
        "\n",
        "– Индустриальные сдвиги: акценты на инклюзивность и смену креативных директоров как факторы, влияющие на позиционирование брендов и коммуникационные стратегии.\n",
        "\n",
        "– Микротренды: закрепление отдельных мотивов (шаровары, джинсовые силуэты baggy/wide leg, аксессуарные акценты; в т. ч. «ретро-тех» – проводные наушники) в качестве повторяющихся сигналов.\n",
        "\n",
        "– Культурные драйверы: влияние селебрити и поп-культуры (например, Blackpink), а также ностальгии по 1990–2000-ым; распространение эстетики «grandma chic».\n",
        "\n",
        "###**Заключение**\n",
        "\n",
        "Векторный поиск по эмбеддингам корректно выделяет тематическое ядро «мода и тренды» и соседние индустриальные/культурные аспекты, что подтверждается устойчивым присутствием сезонных обзоров и специфических микротрендов. Наличие сопутствующих нерелевантных фрагментов объясняется укрупнёнными единицами анализа (чанками), внутри которых совмещаются разные тематические блоки одного материала.\n",
        "\n",
        "\n",
        "Для улучшения получаемых результатов детализируем запросы и, при необходимости, добавим пост-фильтрацию по ключевым словам или теме.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvCNnxanHJIJ"
      },
      "source": [
        "# Тематический анализ корпуса\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsH_zpl0QH6u"
      },
      "source": [
        "## Выбор тем и детализация запросов\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Исходя из цели проекта и специфики телеграм-канала **The Blueprint**, выберем три \"рубрики\" для выявления ключевых тем корпуса:\n",
        "\n",
        "1) \"Модная одежда и тренды\";\n",
        "2) \"Новости культуры\";\n",
        "3) \"Красота и здоровье\"."
      ],
      "metadata": {
        "id": "2V9ydgoyZhOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Модная одежда и тренды\" # Вводим запрос\n",
        "top_k = 15 # Выбираем, какое количество результатов нужно отобразить\n",
        "vector = model.encode([query]) # Кодируем вектор\n",
        "\n",
        "# Производим поиск\n",
        "distances, indices = faiss_index.search(vector, top_k)\n",
        "\n",
        "# Создаем каталог результатов\n",
        "results = []\n",
        "for i, idx in enumerate(indices[0]):\n",
        "        results.append({\n",
        "            \"chunk\": chunks[idx],\n",
        "            \"score\": float(distances[0][i])\n",
        "        })\n",
        "\n",
        "# Выводим первый результат с ответом на вопрос\n",
        "print(\"Top results for the query:\")\n",
        "for i, result in enumerate(results, 1):\n",
        "  print(f\"{i}. Chunk: {result['chunk']}\\nScore: {result['score']}\\n{'-'*40}\")"
      ],
      "metadata": {
        "id": "LNMaFnO_adnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Новости культуры\" # Вводим запрос\n",
        "top_k = 15 # Выбираем, какое количество результатов нужно отобразить\n",
        "vector = model.encode([query]) # Кодируем вектор\n",
        "\n",
        "# Производим поиск\n",
        "distances, indices = faiss_index.search(vector, top_k)\n",
        "\n",
        "# Создаем каталог результатов\n",
        "results = []\n",
        "for i, idx in enumerate(indices[0]):\n",
        "        results.append({\n",
        "            \"chunk\": chunks[idx],\n",
        "            \"score\": float(distances[0][i])\n",
        "        })\n",
        "\n",
        "# Выводим первый результат с ответом на вопрос\n",
        "print(\"Top results for the query:\")\n",
        "for i, result in enumerate(results, 1):\n",
        "  print(f\"{i}. Chunk: {result['chunk']}\\nScore: {result['score']}\\n{'-'*40}\")"
      ],
      "metadata": {
        "id": "5_s6q4vSbZ9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Красота и здоровье\" # Вводим запрос\n",
        "top_k = 15 # Выбираем, какое количество результатов нужно отобразить\n",
        "vector = model.encode([query]) # Кодируем вектор\n",
        "\n",
        "# Производим поиск\n",
        "distances, indices = faiss_index.search(vector, top_k)\n",
        "\n",
        "# Создаем каталог результатов\n",
        "results = []\n",
        "for i, idx in enumerate(indices[0]):\n",
        "        results.append({\n",
        "            \"chunk\": chunks[idx],\n",
        "            \"score\": float(distances[0][i])\n",
        "        })\n",
        "\n",
        "# Выводим первый результат с ответом на вопрос\n",
        "print(\"Top results for the query:\")\n",
        "for i, result in enumerate(results, 1):\n",
        "  print(f\"{i}. Chunk: {result['chunk']}\\nScore: {result['score']}\\n{'-'*40}\")"
      ],
      "metadata": {
        "id": "2isVZVN_bj0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "********"
      ],
      "metadata": {
        "id": "qVCVZyCic6JU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Что видно по выдачам:"
      ],
      "metadata": {
        "id": "eQQ35cnUo3n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Модная одежда и тренды**\n",
        "\n",
        "Доминирующие темы:\n",
        "\n",
        "Подиум-дайджесты AW’25/26 / Resort: сводки трендов после недель моды (NY–London–Milan–Paris).\n",
        "\n",
        "Микротренды сезона:\n",
        "шаровары (от Chloé/Alaïa/Valentino), «grandma chic», проводные наушники как аксессуар, шорты (бермуды/микро/джорты), летняя обувь (вьетнамки, рыбацкие сандалии, «домашние» силуэты).\n",
        "\n",
        "Деним: устойчивость широких силуэтов (baggy/boyfriend/wide-leg) vs попытки вернуть скинни (Miu Miu/Prada).\n",
        "\n",
        "Индустрия и инфлюенс: доминирование люксов на красных дорожках, эффект гостей/селебрити (Saint Laurent, Blackpink), «мид-сайз» вместо plus-size.\n",
        "\n",
        "**Выводы:**\n",
        "\n",
        "Весна 2025 у The Blueprint – комбо подиумных итогов + ностальгии 90/00-х + практики гардероба. Много разговоров о пиар-эффектах брендов, а также о сдвиге инклюзии (мид-сайз).\n",
        "\n",
        "___\n",
        "\n",
        "**2) Новости культуры**\n",
        "\n",
        "Доминирующие темы:\n",
        "\n",
        "Выставки и музеи: MMOMA, галерея Алины Пинской, Эрмитаж (ар-деко), Тейт Модерн 25 лет, гиды по музеям (в т.ч. Япония).\n",
        "\n",
        "Фестивали/ивенты: Met Gala как культурный медиа-ивент, Каннский кинофестиваль, Евровидение, биеннале/фестивали медиаарта (Казань).\n",
        "\n",
        "Книги и ярмарки: non/fiction, списки новинок.\n",
        "\n",
        "Музыка/кино: обзоры релизов и туров; авторские разборы (Matthew Rankin).\n",
        "\n",
        "Технологии & право: заметка про авторское право и ИИ.\n",
        "\n",
        "**Выводы:**\n",
        "\n",
        "Рубрика строится вокруг календаря ивентов с явными пиками в мае (Cannes/Met Gala/Eurovision) и сильной долей московской институциональной сцены.\n",
        "\n",
        "___\n",
        "\n",
        "**3) Красота и здоровье**\n",
        "\n",
        "Доминирующие темы:\n",
        "\n",
        "Уход за кожей: SPF/санскрин, увлажнение и водный баланс, минималистичный уход, мужской уход, посты с рекомендациями редакции.\n",
        "\n",
        "Дерматология vs тренды TikTok: «не умываться/skin fasting» – позиция дерматологов, мифы и факты.\n",
        "\n",
        "Макияж/волосы/тело: «французский» минимализм, ретро-укладки для красных дорожек, средства для сияния тела/автозагар.\n",
        "\n",
        "Индустрия: «самые перспективные бьюти-бренды», эффект амбассадоров.\n",
        "\n",
        "Ментальное здоровье: пост к Дню борьбы с депрессией.\n",
        "\n",
        "**Выводы:**\n",
        "\n",
        "Фокус на прагматичном уходе и просвещении, а не только новинках. Большой пласт – развенчание спорных практик из социальных сетей."
      ],
      "metadata": {
        "id": "QbK5VTW4o1Rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Построение графиков"
      ],
      "metadata": {
        "id": "KLCDlXJiuF6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Бар-чарты:"
      ],
      "metadata": {
        "id": "C1umhN-H5pmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для каждой рубрики для наглядности построим по 12 би-грамм:"
      ],
      "metadata": {
        "id": "AmnaimXOqn2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def topk_chunks(query, k=40):\n",
        "    qv = model.encode([query], normalize_embeddings=True)\n",
        "    D, I = faiss_index.search(qv, k)\n",
        "    return [chunks[i] for i in I[0]]\n",
        "\n",
        "STOP_UNI = set(\"\"\"\n",
        "и в на по за от для о к у с а но или что как уже еще ещё это этот эта этом\n",
        "которые который какие какой чтобы можно нужно будет самый самая самые такой такая такие\n",
        "вот очень просто более менее после перед при без со также однако где когда почему потому\n",
        "\"\"\".split())\n",
        "\n",
        "STOP_BI = {\n",
        "    \"потому что\",\"специально для\",\"уже сегодня\",\"в этом\",\"этом году\",\n",
        "    \"что они\",\"как они\",\"лето вот\",\"вот лучших\",\"после сериала\",\"больше всего\",\n",
        "    \"сегодня вечером\",\"дайджесте главных\",\"обратить внимание\",\"тем самым\", \"этой неделе\",\n",
        "    \"неделе моды\", \"моды париже\", \"недель моды\", \"музее современного\", \"материалов этой\", \"один самых\",\n",
        "    \"меньше чем\", \"одной самых\", \"главных материалов\", \"самых ожидаемых\", \"интервью директором\", \"амирова собрала\",\n",
        "    \"института костюма\", \"прошедших недель\", \"собрали главные\", \"последние годы\", \"день рождения\", \"чем тут\",\n",
        "    \"нашей эры\", \"ищете идеальный\", \"главных тренда\", \"всё что\", \"нужно знать\", \"специально для\", \"уже сегодня\"\n",
        "}\n",
        "\n",
        "def keep_bigram(bg: str) -> bool:\n",
        "    if bg in STOP_BI: return False\n",
        "    w = bg.split()\n",
        "    if len(w) != 2: return False\n",
        "    w1, w2 = w\n",
        "    if len(w1) < 3 or len(w2) < 3: return False\n",
        "    if (w1 in STOP_UNI) or (w2 in STOP_UNI): return False\n",
        "    return True\n",
        "\n",
        "def top_bigrams_filtered(texts, top_n=12, min_df=3):\n",
        "    vec = TfidfVectorizer(\n",
        "        ngram_range=(2, 2),\n",
        "        token_pattern=r\"(?u)\\b[а-яa-zё\\-]{3,}\\b\",\n",
        "        stop_words=None,\n",
        "        min_df=min_df\n",
        "    )\n",
        "    X = vec.fit_transform(texts)\n",
        "    terms = vec.get_feature_names_out()\n",
        "    scores = np.asarray(X.sum(axis=0)).ravel()\n",
        "    pairs = [(t, s) for t, s in zip(terms, scores) if keep_bigram(t)]\n",
        "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # фолбэк: если мало — ослабляем отсев\n",
        "    if len(pairs) < top_n:\n",
        "        vec.set_params(min_df=1)\n",
        "        X = vec.fit_transform(texts)\n",
        "        terms = vec.get_feature_names_out()\n",
        "        scores = np.asarray(X.sum(axis=0)).ravel()\n",
        "        pairs = [(t, s) for t, s in zip(terms, scores) if keep_bigram(t)]\n",
        "        pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return pairs[:top_n]\n",
        "\n",
        "RUBRICS = [\"Модная одежда и тренды\",\"Новости культуры\",\"Красота и здоровье\"]\n",
        "\n",
        "for q in RUBRICS:\n",
        "    texts = topk_chunks(q, 40)          # ← реально берём 40\n",
        "    pairs = top_bigrams_filtered(texts, top_n=12, min_df=3)\n",
        "    print(f\"\\n=== {q} — топ биграмм ===\")\n",
        "    for t, s in pairs:\n",
        "        print(f\"{t}  ({s:.4f})\")"
      ],
      "metadata": {
        "id": "vZ2Jw9_yuJjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отберём наиболее релевантные би-граммы и построим бар-чарты:"
      ],
      "metadata": {
        "id": "Jvrrr6Lyy9V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1) вручную фиксируем выбранные биграммы и их веса\n",
        "data = {\n",
        "    \"Модная одежда и тренды\": {\n",
        "        \"dries van noten\": 0.3100 + 0.3100,  # склейка биграмм\n",
        "        \"tom ford\": 0.4391,\n",
        "        \"saint laurent\": 0.3289,\n",
        "        \"louis vuitton\": 0.3069,\n",
        "        \"стиле бохо\": 0.2834,\n",
        "    },\n",
        "    \"Новости культуры\": {\n",
        "        \"met gala\": 0.7812,\n",
        "        \"современного искусства\": 0.4407,\n",
        "        \"каннского кинофестиваля\": 0.2766,\n",
        "        \"театральные премьеры\": 0.2551,\n",
        "        \"алины пинской\": 0.2223,\n",
        "    },\n",
        "    \"Красота и здоровье\": {\n",
        "        \"tom ford\": 0.2693,\n",
        "        \"эльзы перетти\": 0.2010,\n",
        "        \"миучча прада\": 0.1971,\n",
        "        \"идеальный санскрин\": 0.1936,\n",
        "        \"санскрин лето\": 0.1936,\n",
        "    },\n",
        "}\n",
        "\n",
        "# 2) рисуем три горизонтальных бар-чарта\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "for ax, (title, d) in zip(axes, data.items()):\n",
        "    # сортируем по убыванию\n",
        "    items = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
        "    labels = [k for k, _ in items]\n",
        "    scores = [v for _, v in items]\n",
        "    y = np.arange(len(labels))[::-1]\n",
        "    ax.barh(y, scores[::-1])\n",
        "    ax.set_yticks(y)\n",
        "    ax.set_yticklabels(labels[::-1])\n",
        "    ax.set_title(title)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2TFxqzjMzJgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из выделенных би-граммов и соответствующих бар-чартов, возможно сделать выводы:\n",
        "\n",
        "Мода\n",
        "\n",
        "Фокус брендоцентричный: Saint Laurent, Tom Ford, Louis Vuitton, Dries Van Noten – во главе.\n",
        "\n",
        "Есть эстетики/предметы (стиль бохо, «неделя моды») – тренд-репорты и ретроспектива показов.\n",
        "\n",
        "Вывод: весной канал обсуждает коллекции и силуэты топ-домов; тренды формулируются через кейсы брендов и итоги недель моды.\n",
        "\n",
        "___\n",
        "\n",
        "Культура\n",
        "\n",
        "Главный драйвер – крупные ивенты: Met Gala доминирует; рядом современное искусство (музеи/выставки), театр, Каннский кинофестиваль.\n",
        "\n",
        "Вывод: «Культура» в канале – это пересечение моды и арта: модные события (Met Gala) трактуются как культурные, плюс стабильный поток выставок/премьер.\n",
        "\n",
        "___\n",
        "\n",
        "Красота\n",
        "\n",
        "Сигналы сезонного ухода: идеальный санскрин/санскрин лето.\n",
        "\n",
        "Смешение с модной повесткой (имена вроде Tom Ford, Миучча Прада, Эльза Перетти), поскольку бьюти-темы часто привязаны к модным персонам/домам.\n",
        "\n",
        "Вывод: весной – практический контент про SPF и лёгкий уход, но бьюти продолжает мигрировать из модной ленты."
      ],
      "metadata": {
        "id": "hwSqfEUw5AXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Теплокарта \"биграмма × рубрика\""
      ],
      "metadata": {
        "id": "ibEdIO9h5jYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# списки для чистки\n",
        "STOP_UNI = set(\"\"\"и в на по за от для о к у с а но или что как уже еще ещё это этот эта этом которые который какие какой чтобы можно нужно будет самый самая самые такой такая такие вот очень просто более менее после перед при без со однако где когда почему потому\"\"\".split())\n",
        "STOP_BI = {\n",
        "    \"потому что\",\"специально для\",\"уже сегодня\",\"в этом\",\"этом году\",\"что они\",\"как они\",\n",
        "    \"лето вот\",\"вот лучших\",\"после сериала\",\"больше всего\",\"сегодня вечером\",\"дайджесте главных\",\n",
        "    \"обратить внимание\",\"тем самым\",\"этой неделе\",\"неделе моды\",\"моды париже\",\"недель моды\",\n",
        "    \"музее современного\",\"материалов этой\",\"один самых\",\"меньше чем\",\"одной самых\",\"главных материалов\",\n",
        "    \"самых ожидаемых\",\"интервью директором\",\"амирова собрала\",\"института костюма\",\"прошедших недель\",\n",
        "    \"собрали главные\",\"последние годы\",\"день рождения\",\"чем тут\",\"нашей эры\",\"ищете идеальный\",\n",
        "    \"главных тренда\",\"всё что\",\"нужно знать\", \"современное искусство\", \"современного искусства\", \"тех пор\",\n",
        "    \"креативного директора\", \"креативных директоров\", \"массажем вашей\", \"отправиться лучшим\", \"тех кто\", \"этой недели\",\n",
        "    \"самом деле\", \"совсем недавно\", \"стоит обратить\", \"своими любимыми\", \"самом деле\", \"лучшим массажем\", \"куда отправиться\",\n",
        "    \"том числе\", \"знать про\", \"нашем материале\", \"зарубежных материалов\", \"косметику куда\", \"самых интересных\", \"нижнем новгороде\",\n",
        "    \"дайджесте зарубежных\", \"каждый день\", \"телеграм-канала вишлист\", \"смотреть мае\", \"помощью нашего\", \"весной летом\", \"всему миру\",\n",
        "    \"эти вопросы\", \"этой весной\", \"этой роли\", \"предметы одежды\", \"японии кроме\", \"вашей жизни\"\n",
        "}\n",
        "\n",
        "def _normalize_text(s: str) -> str:\n",
        "    # унифицируем пробелы/тире и приводим к нижнему регистру\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[–—−]\", \"-\", s)           # все длинные тире -> \"-\"\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()      # схлопываем пробелы\n",
        "    return s\n",
        "\n",
        "def _keep_bigram(bg: str) -> bool:\n",
        "    if bg in STOP_BI:\n",
        "        return False\n",
        "    w = bg.split()\n",
        "    if len(w) != 2:\n",
        "        return False\n",
        "    return all((len(t) >= 3 and t not in STOP_UNI) for t in w)\n",
        "\n",
        "def bigram_docshare(texts, topn=8, min_df=2):\n",
        "    # нормализуем тексты заранее, чтобы термины совпадали со STOP_BI\n",
        "    texts = [_normalize_text(t) for t in texts]\n",
        "    cv = CountVectorizer(\n",
        "        ngram_range=(2, 2),\n",
        "        token_pattern=r\"(?u)\\b[а-яa-zё\\-]{3,}\\b\",\n",
        "        min_df=min_df,\n",
        "        binary=True,\n",
        "        lowercase=False  # понизили регистр\n",
        "    )\n",
        "    X = cv.fit_transform(texts)          # docs × terms\n",
        "    terms = cv.get_feature_names_out()\n",
        "\n",
        "    # применяем фильтр сразу к словарю\n",
        "    mask = np.array([_keep_bigram(t) for t in terms])\n",
        "    if mask.any():\n",
        "        X = X[:, mask]\n",
        "        terms = terms[mask]\n",
        "\n",
        "    df = np.asarray(X.sum(axis=0)).ravel()\n",
        "    idx = df.argsort()[-topn:][::-1]\n",
        "    share = (df[idx] / X.shape[0])\n",
        "    return dict(zip(terms[idx], share)), X.shape[0]\n",
        "\n",
        "RUBRICS = [\n",
        "    (\"Модная одежда и тренды\", 30),\n",
        "    (\"Новости культуры\", 30),\n",
        "    (\"Красота и здоровье\", 30),\n",
        "]\n",
        "\n",
        "# 1) собираем top биграммы по рубрикам\n",
        "per_rubric = {}\n",
        "sizes = {}\n",
        "for name, K in RUBRICS:\n",
        "    T = topk_chunks(name, K)\n",
        "    d, n = bigram_docshare(T, topn=8, min_df=2)\n",
        "    per_rubric[name] = d; sizes[name] = n\n",
        "\n",
        "# 2) объединяем словарь биграмм (строки матрицы)\n",
        "vocab = sorted(set().union(*[d.keys() for d in per_rubric.values()]))\n",
        "\n",
        "# 3) строим матрицу долей и рисуем\n",
        "M = np.zeros((len(vocab), len(RUBRICS)))\n",
        "for j, (name, _) in enumerate(RUBRICS):\n",
        "    for i, term in enumerate(vocab):\n",
        "        M[i, j] = per_rubric[name].get(term, 0.0)\n",
        "\n",
        "plt.figure(figsize=(10, 0.5*len(vocab)+2))\n",
        "im = plt.imshow(M, aspect=\"auto\")\n",
        "plt.xticks(range(len(RUBRICS)), [r[0] for r in RUBRICS], rotation=0)\n",
        "plt.yticks(range(len(vocab)), vocab)\n",
        "plt.colorbar(label=\"Доля документов (0–1)\")\n",
        "# подписи чисел внутри клеток\n",
        "for i in range(len(vocab)):\n",
        "    for j in range(len(RUBRICS)):\n",
        "        v = M[i, j]\n",
        "        if v > 0:\n",
        "            plt.text(j, i, f\"{v:.2f}\", ha=\"center\", va=\"center\", fontsize=8)\n",
        "plt.title(\"Биграммы × рубрики (документные доли)\")\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "xNit3K6l5yZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По данной тепловой карте можно сделать определённые выводы относительно того, что было популярно и о чём идет речь в постах The Blueprint в соответствии с каждой из трёх рубрик (Бренды, Met Gala, свадебные тренды, броши, Юра Борисов и т.д.).\n",
        "\n",
        "Главным образом можно заметить не самые логично выделяющиеся биграммы в рубрике \"Красота и здоровье\" - это позволяет сделать вывод о том, что запрос нужно корректировать для отражения конкретных трендов за весну в этой сфере на графиках."
      ],
      "metadata": {
        "id": "U5-tacql98rR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Тайм-лайн (пики по неделям)"
      ],
      "metadata": {
        "id": "_xmc8V2a6dDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Таймлайн упоминаний по неделям (март–май)\n",
        "import json, re, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Загрузка и нормализация текста\n",
        "def flatten_text(t):\n",
        "    if isinstance(t, list):\n",
        "        out = []\n",
        "        for el in t:\n",
        "            out.append(el if isinstance(el, str) else el.get(\"text\", \"\"))\n",
        "        return \"\".join(out)\n",
        "    return t if isinstance(t, str) else \"\"\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[–—−]\", \"-\", s)      # длинные тире -> \"-\"\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "with open(\"result.json\",\"r\",encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "rows = []\n",
        "for m in data[\"messages\"]:\n",
        "    if isinstance(m, dict) and \"date\" in m and \"text\" in m:\n",
        "        txt = normalize_text(flatten_text(m[\"text\"]))\n",
        "        rows.append({\"date\": pd.to_datetime(m[\"date\"]), \"text\": txt})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df = df[(df[\"date\"] >= \"2025-03-01\") & (df[\"date\"] <= \"2025-05-31\")].copy()\n",
        "\n",
        "# 2) Темы\n",
        "TOPICS = {\n",
        "    # Ивенты\n",
        "    \"Met Gala\": [\n",
        "        r\"\\bmet\\s*gala\\b\",\n",
        "        r\"\\bбал(?:\\s+|-)института\\b\",\n",
        "        r\"\\bинститута\\s+костюма\\b\",\n",
        "    ],\n",
        "    \"Каннский кинофестиваль\": [\n",
        "        r\"каннск\\w+\\s+кинофест\\w+\",\n",
        "        r\"\\bcannes\\b\",\n",
        "    ],\n",
        "    \"Недели моды\": [\n",
        "        r\"недел[ьяи]\\s+мод[ыаи]\",\n",
        "        r\"\\bfashion(?:\\s+|-)week\\b\",\n",
        "    ],\n",
        "\n",
        "    # Бренды\n",
        "    \"Louis Vuitton\": [\n",
        "        r\"\\blouis\\s+vuitton\\b\", r\"\\blv\\b\",\n",
        "    ],\n",
        "    \"Saint Laurent\": [\n",
        "        r\"\\bsaint\\s+laurent\\b\", r\"\\bysl\\b\",\n",
        "    ],\n",
        "    \"Miu Miu\": [\n",
        "        r\"\\bmiu\\s*miu\\b\",\n",
        "    ],\n",
        "\n",
        "    # Микротренды\n",
        "    \"Шаровары\": [\n",
        "        r\"шаровар\\w+\",\n",
        "    ],\n",
        "    \"Проводные наушники\": [\n",
        "        r\"проводн\\w+\\s+наушник\\w+\",\n",
        "    ],\n",
        "\n",
        "    # Бьюти\n",
        "    \"SPF/санскрин\": [\n",
        "        r\"\\bspf\\b\", r\"санскрин\", r\"солнцезащит\\w+\",\n",
        "    ],\n",
        "    \"Уход за кожей\": [\n",
        "        r\"уход\\s+за\\s+кож\\w+\", r\"\\bсыворотк\\w+\\b\", r\"\\bкрем\\w*\\b\", r\"\\bпор[аы]\\b\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# 3) Подсчёт по неделям\n",
        "start = df[\"date\"].min().floor(\"D\")\n",
        "end   = df[\"date\"].max().floor(\"D\")\n",
        "weekly_index = pd.date_range(start, end, freq=\"W-MON\")  # недели, завершающиеся в понедельник\n",
        "\n",
        "timeline = pd.DataFrame(index=weekly_index)\n",
        "\n",
        "for name, pats in TOPICS.items():\n",
        "    mask = pd.Series(False, index=df.index)\n",
        "    for p in pats:\n",
        "        mask |= df[\"text\"].str.contains(p, regex=True, case=False)\n",
        "    s = df.loc[mask].groupby(pd.Grouper(key=\"date\", freq=\"W-MON\")).size()\n",
        "    timeline[name] = s.reindex(weekly_index, fill_value=0)\n",
        "\n",
        "# 4) Один график: только топ-N тем\n",
        "TOP_N = 8  # оставляем N самых «громких» линий\n",
        "keep = timeline.sum(0).sort_values(ascending=False).head(TOP_N).index\n",
        "plot_df = timeline[keep].rolling(window=2, min_periods=1).mean()  # лёгкое сглаживание\n",
        "\n",
        "ax = plot_df.plot(figsize=(11,5), linewidth=2)\n",
        "ax.set_title(\"Упоминания по неделям (март–май): топ тем\")\n",
        "ax.set_ylabel(\"Постов в неделю\"); ax.set_xlabel(\"\")\n",
        "ax.grid(True, alpha=0.25); ax.margins(x=0)\n",
        "ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", frameon=False, title=\"Темы\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "# 4) Мини-графики по группам\n",
        "groups = {\n",
        "    \"Ивенты\": [\"Met Gala\",\"Каннский кинофестиваль\",\"Недели моды\"],\n",
        "    \"Бренды\": [\"Louis Vuitton\",\"Saint Laurent\",\"Tom Ford\",\"Miu Miu\",\"Valentino\",\"Balenciaga\",\"Dries Van Noten\"],\n",
        "    \"Бьюти\":  [\"SPF/санскрин\",\"Уход за кожей\",\"Минималистичный макияж\"],\n",
        "    \"Микротренды\": [\"Шаровары\",\"Проводные наушники\",\"Рыбацкие сандалии\",\"Grandma chic\",\"Шорты (лето)\"],\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12,6), sharex=True, sharey=True)\n",
        "axes = axes.ravel()\n",
        "\n",
        "for ax, (gname, topics) in zip(axes, groups.items()):\n",
        "    cols = [t for t in topics if t in timeline.columns]\n",
        "    if not cols:\n",
        "        ax.axis(\"off\"); continue\n",
        "    T = timeline[cols]\n",
        "    # показываем топ-3 внутри группы\n",
        "    top_in_group = T.sum(0).sort_values(ascending=False).head(3).index\n",
        "    T[top_in_group].rolling(2, min_periods=1).mean().plot(ax=ax, linewidth=2)\n",
        "    ax.set_title(gname); ax.grid(True, alpha=0.25); ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
        "\n",
        "fig.suptitle(\"Упоминания по неделям (март–май): сгруппировано\", y=1.02)\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "GGNCzZEB6f9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Согласно представленным графикам:\n",
        "\n",
        "Ивенты:\n",
        "\n",
        "Met Gala – мощный пик во 2-й неделе мая (это и есть ночь Met Gala). До и после – заметные «хвосты» обсуждений.\n",
        "\n",
        "Недели моды – максимумы приходятся на середину марта, затем плавный спад к апрелю.\n",
        "\n",
        "Канны – рост начинается с середины мая, пик – конец мая.\n",
        "\n",
        "👉 Лента двигается по календарю индустрии: март – дайджесты FW, начало мая – Met Gala, конец мая – Канны.\n",
        "\n",
        "___\n",
        "\n",
        "Бренды:\n",
        "\n",
        "Louis Vuitton / Saint Laurent / Miu Miu – низкий, «фоновый» уровень с короткими всплесками (скорее привязка к ивентам и к отдельным материалам, чем самостоятельные инфоповоды).\n",
        "\n",
        "Небольшой пик LV в конце апреля/начале мая может совпадать с обзорами коллекций/проектов.\n",
        "\n",
        "___\n",
        "\n",
        "Бьюти:\n",
        "\n",
        "Уход за кожей – стабильные 1–2 поста в неделю почти весь период.\n",
        "\n",
        "SPF/санскрин – сезонная динамика: подрастает в мае и достигает максимума перед летом. Это ожидаемая тема к жаре.\n",
        "\n",
        "___\n",
        "\n",
        "Микротренды\n",
        "\n",
        "Проводные наушники – еле заметный «огонёк» в конце мая.\n",
        "\n",
        "Шаровары – в таймлайне почти нет (тема яркая, но точечная: 1-2 лонгрида, а не частые упоминания).\n",
        "\n",
        "___\n",
        "\n",
        "Общий вывод:\n",
        "\n",
        "Редакционный фокус закономерен: март – итоги недель моды → май – Met Gala → конец мая – Канны.\n",
        "Бьюти-темы держатся фоном, SPF – выраженный сезонный пик. Бренды и микротренды упоминаются преимущественно в связке с ивентами."
      ],
      "metadata": {
        "id": "6MkG_MV2DwVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Итоги контент-анализа The Blueprint (март–май 2025)"
      ],
      "metadata": {
        "id": "fiSn51P1Xu1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ключевые результаты"
      ],
      "metadata": {
        "id": "OXJcI4wzfp0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Методика: векторный поиск (FAISS + paraphrase-multilingual-mpnet-base-v2) дал релевантные срезы без ручной разметки.\n",
        "\n",
        "---\n",
        "\n",
        "Рубрики и фокус:\n",
        "\n",
        "Мода: устойчивые упоминания Louis Vuitton / Saint Laurent / Tom Ford; прикладные подборки (шорты/обувь); микротренды (шаровары, проводные наушники); эстетика grandma chic.\n",
        "\n",
        "Культура: крупные инфоповоды – Met Gala, Каннский кинофестиваль; регулярные обзоры современного искусства и театральных премьер.\n",
        "\n",
        "Красота: стабильный фон по уходу за кожей; выраженная сезонность SPF/санскрин (рост в мае).\n",
        "\n",
        "---\n",
        "\n",
        "Недельный таймлайн: март – пик «Недель моды» → тренд-репорты; начало–середина мая – всплеск Met Gala; конец мая – рост Канн; SPF плавно растёт к лету.\n",
        "\n",
        "Связи: бренды «подсвечиваются» вокруг ивентов; бьюти усиливается в ивентные недели и по сезону; культура контекстуализирует модные темы.\n",
        "\n",
        "---\n",
        "\n",
        "**Практическая ценность:**\n",
        "\n",
        "Планирование пиков: FW (март) → Met Gala (май-W2) → Канны (май-W3–W4).\n",
        "\n",
        "Под ивенты – бренд-интеграции; к маю – акцент на SPF/летний уход; между пиками – прикладные «что купить/как носить»."
      ],
      "metadata": {
        "id": "G6afNKB2X0nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Направления дальнейшей работы"
      ],
      "metadata": {
        "id": "0C6CHeN5fzBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ограничения исследования:**\n",
        "\n",
        "- Считаются упоминания, а не охваты/просмотры.\n",
        "\n",
        "- Длинные чанки иногда включают «соседние» темы из одного поста.\n",
        "\n",
        "- Пики брендов закономерно коррелируют с календарём ивентов (это учитывалось при интерпретации).\n",
        "\n",
        "**Рекомендации по доработке:**\n",
        "\n",
        "- Чанки: уменьшить до ~600–700 символов (overlap 100–150) для снижения «примесей».\n",
        "\n",
        "- Запросы/выборка: детализировать формулировки, повысить top-k (до 40–60) для устойчивых n-грамм.\n",
        "\n",
        "- Лексика: короткий стоп-лист для биграмм; опционально — лемматизация RU перед n-граммами.\n",
        "\n",
        "- Пост-фильтрация: для «Красоты» требовать маркеры (spf|санскрин|кожа|крем|сыворотк|дерматолог|макияж).\n",
        "\n",
        "- Визуализации: дополнить теплокартой «рубрика × топ-термины» и/или сгруппированным таймлайном (ивенты/бренды/бьюти/микротренды)."
      ],
      "metadata": {
        "id": "Ju24tWs6fxlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TL;DR"
      ],
      "metadata": {
        "id": "B-egMzOUgnLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Краткое резюме:**\n",
        "\n",
        "Весенняя повестка канала следует календарю индустрии; мода – брендоцентрична, рубрика о культуре завязана на новостях и ивентах, сфера красоты и здоровья фокусируется на бьюти-советах, косметике, советов бьюти-гуру и зависит от сезона. Метод дал быстрые и полезные срезы; для «идеала» нужны более мелкие чанки, детальнее запросы и лёгкая пост-фильтрация."
      ],
      "metadata": {
        "id": "Ai59YZ6Wgq4F"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7cpJw7OIvbJn",
        "SssmOWIz7LDf",
        "JRhIaQh-kaqr",
        "Hhv88A088UnW",
        "8W6bOQwPaVGB",
        "GsH_zpl0QH6u",
        "eQQ35cnUo3n9",
        "KLCDlXJiuF6b",
        "C1umhN-H5pmw",
        "fiSn51P1Xu1E",
        "OXJcI4wzfp0L",
        "0C6CHeN5fzBx",
        "B-egMzOUgnLA"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}